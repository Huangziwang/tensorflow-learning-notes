#   运用CNN对mnist数据集进行分类
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(r"MNIST_data/", one_hot=True)

trainimg   = mnist.train.images
trainlabel = mnist.train.labels
testimg    = mnist.test.images
testlabel  = mnist.test.labels
print ("MNIST ready")

n_input=784
n_output=10

weights={
    'wc1': tf.Variable(tf.random_normal([3,3,1,64],stddev=0.1)),
    'wc2': tf.Variable(tf.random_normal([3,3,64,128],stddev=0.1)),
    'wd1': tf.Variable(tf.random_normal([7*7*128,1024],stddev=0.1)),
    'wd2': tf.Variable(tf.random_normal([1024,n_output],stddev=0.1))
}

biases   = {
        'bc1': tf.Variable(tf.random_normal([64], stddev=0.1)),
        'bc2': tf.Variable(tf.random_normal([128], stddev=0.1)),
        'bd1': tf.Variable(tf.random_normal([1024], stddev=0.1)),
        'bd2': tf.Variable(tf.random_normal([n_output], stddev=0.1))
    }

def conv_basic(input,w,b,keepratio):
    input_r=tf.reshape(input,shape=[-1,28,28,1])
    conv1=tf.nn.conv2d(input_r,w['wc1'],strides=[1,1,1,1],padding='SAME')
    conv1=tf.nn.relu(tf.nn.bias_add(conv1,b['bc1']))
    pool1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')
    pool1_dir=tf.nn.dropout(pool1,keepratio)

    conv2=tf.nn.conv2d(pool1_dir,w['wc2'],strides=[1,1,1,1],padding='SAME')
    conv2=tf.nn.relu(tf.nn.bias_add(conv2,b['bc2']))
    pool2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')
    pool2_dir=tf.nn.dropout(pool2,keepratio)

    dense1=tf.reshape(pool2_dir,[-1,w['wd1'].get_shape().as_list()[0]])
    fc1=tf.nn.relu(tf.add(tf.matmul(dense1,w['wd1']),b['bd1']))
    fc1_dir=tf.nn.dropout(fc1,keepratio)
    out=tf.add(tf.matmul(fc1_dir,w['wd2']),b['bd2'])
    return out

print ("CNN READY")

x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_output])
keepratio = tf.placeholder(tf.float32)

_pred = conv_basic(x, weights, biases, keepratio)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=_pred,labels= y))
optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)
_corr = tf.equal(tf.argmax(_pred,1), tf.argmax(y,1))
accr = tf.reduce_mean(tf.cast(_corr, tf.float32))
init = tf.global_variables_initializer()
print ("GRAPH READY")

with tf.Session() as sess:
    sess.run(init)
    training_epochs=40
    batch_size=16
    display_step=2
    for epoch in range(training_epochs):
        avg_cost = 0.
        # total_batch = int(mnist.train.num_examples/batch_size)
        total_batch = 10
        # Loop over all batches
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            # Fit training using batch data
            sess.run(optm, feed_dict={x: batch_xs, y: batch_ys, keepratio: 0.7})
            # Compute average loss
            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keepratio: 1.}) / total_batch

        # Display logs per epoch step
        if epoch % display_step == 0:
            print("Epoch: %03d/%03d cost: %.9f" % (epoch, training_epochs, avg_cost))
            train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys, keepratio: 1.})
            print(" Training accuracy: %.3f" % (train_acc))
            # test_acc = sess.run(accr, feed_dict={x: testimg, y: testlabel, keepratio:1.})
            # print (" Test accuracy: %.3f" % (test_acc))

    print("OPTIMIZATION FINISHED")
