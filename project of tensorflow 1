# 1 利用tensorflow构建Linear Regression
# import tensorflow as tf
# import numpy as np
# import matplotlib.pyplot as plt

# num_points=1000
# xdata=[]
# ydata=[]
# for i in range(num_points):
#     x1=np.random.normal(0.0,1.0)
#     y1=x1*0.1+np.random.normal(0.0,0.003)
#     xdata.append(x1)
#     ydata.append(y1)
#
# plt.figure(1)
# plt.scatter(xdata,ydata)
#
# w=tf.Variable(tf.random_uniform([1],-1.0,1.0))
# b=tf.Variable([1.])
# y=w*xdata+b   #时刻记住w,b的格式，应该同为float，否则会出错
#
# loss=tf.reduce_mean(tf.square(y-ydata))
# train=tf.train.GradientDescentOptimizer(0.1).minimize(loss)
#
# with tf.Session() as sess:
#     sess.run(tf.global_variables_initializer())
#     print("w=",sess.run(w),"b=",sess.run(b),"loss=",sess.run(loss))
#     for i in range(200):
#         sess.run(train)
#         print("w=", sess.run(w), "b=", sess.run(b), "loss=", sess.run(loss))
#     plt.scatter(xdata,ydata)
#     plt.plot(xdata,sess.run(w)*xdata+sess.run(b),'r--')
#     plt.show()





# 2 使用softmax分类器对mnist数据集进行分类
# import tensorflow as tf
# import numpy as np
# import matplotlib.pyplot as plt
# from tensorflow.examples.tutorials.mnist import input_data
#
# mnist=input_data.read_data_sets(r"MNIST_data/",one_hot=True)
#
#
# trainimg=mnist.train.images
# trainlabel=mnist.train.labels
# testimg=mnist.test.images
# testlabel=mnist.test.labels
# print(" type of 'trainimg' is %s"    % (type(trainimg)))
# print(" type of 'trainlabel' is %s"  % (type(trainlabel)))
# print(" type of 'testimg' is %s"     % (type(testimg)))
# print(" type of 'testlabel' is %s"   % (type(testlabel)))
# print(" shape of 'trainimg' is %s"   % (trainimg.shape,))
# print(" shape of 'trainlabel' is %s" % (trainlabel.shape,))
# print(" shape of 'testimg' is %s"    % (testimg.shape,))
# print(" shape of 'testlabel' is %s"  % (testlabel.shape,))
#
# print ("How does the training data look like?")
# nsample = 5
# randidx = np.random.randint(trainimg.shape[0], size=nsample)
# for i in randidx:
#     curr_img   = np.reshape(trainimg[i, :], (28, 28)) # 28 by 28 matrix
#     curr_label = np.argmax(trainlabel[i, :] ) # Label
#     plt.matshow(curr_img, cmap=plt.get_cmap('gray'))
#     plt.title("" + str(i) + "th Training Data "
#               + "Label is " + str(curr_label))
#     print ("" + str(i) + "th Training Data "
#            + "Label is " + str(curr_label))
#     plt.show()
#
# # Batch Learning?
# print ("Batch Learning? ")
# batch_size = 100
# batch_xs, batch_ys = mnist.train.next_batch(batch_size)
# print ("type of 'batch_xs' is %s" % (type(batch_xs)))
# print ("type of 'batch_ys' is %s" % (type(batch_ys)))
# print ("shape of 'batch_xs' is %s" % (batch_xs.shape,))
# print ("shape of 'batch_ys' is %s" % (batch_ys.shape,))
# print (trainimg.shape)
# print (trainlabel.shape)
# print (testimg.shape)
# print (testlabel.shape)
# #print (trainimg)
# print (trainlabel[0])
#
# x=tf.placeholder(tf.float32,[None,784])
# y=tf.placeholder(tf.float32,[None,10])
# W=tf.Variable(tf.random_normal([784,10]))
# b=tf.Variable(tf.random_normal([10]))
#
# actv=tf.nn.softmax(tf.matmul(x,W)+b)
#
# cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(actv),reduction_indices=1))
#
# learning_rate=0.01
# optm=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)
#
# pred=tf.equal(tf.argmax(actv,1),tf.argmax(y,1))
#
# accr=tf.reduce_mean(tf.cast(pred,"float"))
#
# init=tf.global_variables_initializer()
# training_epochs=50
# batch_size=100
# display_step=5
#
# with tf.Session() as sess:
#     sess.run(init)
#     for epoch in range(training_epochs):
#         avg_cost=0.
#         num_batch=int(mnist.train.num_examples/batch_size)
#         for i in range(num_batch):
#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)
#             sess.run(optm, feed_dict={x: batch_xs, y: batch_ys})
#             feeds = {x: batch_xs, y: batch_ys}
#             avg_cost += sess.run(cost, feed_dict=feeds) / num_batch
#             # DISPLAY
#         if epoch % display_step == 0:
#             feeds_train = {x: batch_xs, y: batch_ys}
#             feeds_test = {x: mnist.test.images, y: mnist.test.labels}
#             train_acc = sess.run(accr, feed_dict=feeds_train)
#             test_acc = sess.run(accr, feed_dict=feeds_test)
#             print("Epoch: %03d/%03d cost: %.9f train_acc: %.3f test_acc: %.3f"
#                   % (epoch, training_epochs, avg_cost, train_acc, test_acc))
#     print("DONE")





# 2 使用simple Neural Network 对mnist数据集进行分类
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

mnist=input_data.read_data_sets(r"MNIST_data/",one_hot=True)

n_hidden_1=256
n_hidden_2=128
n_input=784
n_classes=10

x=tf.placeholder(tf.float32,[None,n_input])
y=tf.placeholder(tf.float32,[None,n_classes])

stddev=0.1
weights={
    'w1': tf.Variable(tf.random_normal([n_input,n_hidden_1])),
    'w2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_hidden_2,n_classes]))
}

biases={
    'b1': tf.Variable(tf.random_normal([n_hidden_1])),
    'b2': tf.Variable(tf.random_normal([n_hidden_2])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

print("NetWork Ready")

def multilayer_preceptron(X,Weights,Biases):
    layer1=tf.nn.sigmoid(tf.add(tf.matmul(X,Weights['w1']),Biases['b1']))
    layer2=tf.nn.sigmoid(tf.add(tf.matmul(layer1,Weights['w2']),Biases['b2']))
    return tf.matmul(layer2,Weights['out'])+Biases['out']

pred=multilayer_preceptron(x,weights,biases)

cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
optm=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
corr=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accr=tf.reduce_mean(tf.cast(corr,"float"))

init=tf.global_variables_initializer()
print("Function Ready")

training_epochs=50
batch_size=100
display_step=4

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(mnist.train.num_examples / batch_size)
        # ITERATIONn
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            feeds = {x: batch_xs, y: batch_ys}
            sess.run(optm, feed_dict=feeds)
            avg_cost += sess.run(cost, feed_dict=feeds)
        avg_cost = avg_cost / total_batch
        # DISPLAY
        if (epoch + 1) % display_step == 0:
            print("Epoch: %03d/%03d cost: %.9f" % (epoch, training_epochs, avg_cost))
            feeds = {x: batch_xs, y: batch_ys}
            train_acc = sess.run(accr, feed_dict=feeds)
            print("TRAIN ACCURACY: %.3f" % (train_acc))
            feeds = {x: mnist.test.images, y: mnist.test.labels}
            test_acc = sess.run(accr, feed_dict=feeds)
            print("TEST ACCURACY: %.3f" % (test_acc))
    print("OPTIMIZATION FINISHED")
